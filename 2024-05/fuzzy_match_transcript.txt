There we go without further ado. Hi, everybody. Welcome back to another healthy live demo. Thanks for sticking with us through the time change. I'm glad it looks like most people have been okay with that. Today, we're going to be doing a demo on fuzzy matching. We're going to be talking about it in doing fuzzy matching in different ways and different ways that you might want to match non perfect strings. I'm here today with Ewen Harrison, Professor Ewen Harrison, who is sharing his screen and he's going to be the one that's going to be live coding for us, and I'm going to be talking through some tasks and approaches that you can take. Ewen, if you would be able to in the files tab, there should be a quarto document called Fuzzy Match. That one right there. Can people see? You can zoom in a little bit. If you make the files and environment slightly smaller, yeah. That looks big if anyone want it bigger, let us know. But Anyone that's not super familiar with the posit Cloud, interface. This is just what typical R studio might look like when it's locally on your machine, but we're using a cloud space. It's got the usual four panels, although you and has currently minimised the console so that we can look at the top left panel, which is the script. That's the console down below, which you can minimise by clicking that button there. On the right, we've got our files and then the environment just above. We can switch through a bunch of different tabs on the bottom right but files, that's what's up at the moment. Great. Today, for our fuzzy matching. Instead of using an external dataset, I've created two data frames which are slightly smaller so that we're going to be able to look at the exact differences. But of course, these techniques can be used on bigger datasets though as you'll see, it can get quite tricky if you try and do things that are a bit too big. So it's always worth being aware with what task you're trying to tackle. If you just run the first chunk in, it loads the libraries we need, and then also creates those datasets. Perfect. Yes. The two data sets. If you just have a look through them, you'll see that they've both got names, but they have slightly different information in them otherwise. The first data frame maybe has some more combitis, their age, BMI, things like that. And the second data frame has some medication that the patient is on and also oxygen saturation. Basically, what we're going to do is we're going to try and join these datasets together. So we're going to start with that and then we will see what happens after there and we can discuss the different approaches in terms of if things aren't exact matches. So, if you wouldn't mind trying to just join the datasets either an inner join or a left join, just so that we can see how that works. You can hear me. I'm going to use tidyverse principles to join the two datasets. In this box, I'm going to put data frame one and I'm copying them so I get the names right. Data Frame two. I'm just going to run them to make sure that I've got that syntax. There's data frame one coming up. Data Frame two coming up. Now I've been tipped with inner join or left. Left join is what I would normally start with. I'm I'm using the tiny verse version of the pipe, which is control shift and enter. I'm going to I'll just use the exact name there. I'm going to put that in like that. I'm just going to run that like that. Now I get this thing saying joining by join by name. If I was being proper about it, I would include that in there so that I'm specifying how it gets joined. That's really important if you have got common names across the two datasets, which isn't the case here. The first thing I can do when I look at this is just see what the size of it is. This is 20 by eight, and this is 20 long, and this is long and this is two columns 34, five, six, seven. It looks like it might have worked, but I'm guessing it hasn't. Yeah. It has the concept of a left join is it's taking all of the rows within the left or the first data frame that we're using. In this instance, it's data frame one. And it is then taking the matches from data frame two and putting them together. So if you within that join that you did, if you scroll along to there, you'll see that there's some A in the data frame from the variables that didn't have an exact match from the first data frame. Yeah, exactly. Because we know this is a demo on fuzzy matching. This was done on purpose. I didn't want them all to match. On useful thing I use for finding out the variables that might not be matching is using anti join, which is follows similar syntax to an inner join or a left join or a join, except it uses anti join instead. What it does is it shows the rows that aren't full matches between the two data frames. That's a great thing to do in this situation. Yeah. Here we've got the eight rows from data frame one, which didn't have a match in data frame two. It's a quick tip here that you can see, these are eight rows that didn't fit into my join, and then you can start investigating that might be. An important thing, I think to note is that in this instance, you might then depending what the variable is and use some data wrangling to fix the spelling mistakes as opposed to doing what we're going to do over the course of this demo is try to do fuzzy joining basing on similarities. You could correct your data frame. If there's few enough mistakes, for example, obviously can get quite tricky if you have thousands of rows and you can't necessarily individually fix them all. But just something to bear in mind that I would recommend doing data wrangling and cleaning and those things first. Before we go into other fuzzy joints. One thing to help in this kind of thing is regular expressions. These can be quite tricky. We have done a demo on string R in the past and that covered a lot of regular expressions. I thought we could quickly try and show an example of a regular expression before we move on. No by the names, but by the medications that in data frame two. There is a lot of spelling mistakes of the word paracetamol. Yeah. It's just this is a simple spelling mistake that I did in in the script, there's always just y, it's that letter, that or MO or L or nothing at all. This is a simple spelling mistake. But I guess this is also to show that it can quickly get quite complicated and it can be hard to encompass all of the possible variations that you might get from a spelling mistake in a regular expression. But, if you wanted to give that a go with using a mutate take to change the variable medication and use string string R. STR detect with an I was going to suggest a case when, but if you had another suggestion. And you're testing me. Okay. Let me see if I can get it working first of all, so I'm just going to use old style string detect medication. I don't know if something like that. That's detecting all of them that have parasite. I don't carry regular expressions in my head, and I really need to google like crazy when I'm doing this I find it quite difficult. But this is a regular expression that takes past and then takes any other character, which is the any number of times following that, and string de pe a tru false. It's a logical function based on whether the conditions of this regular expression have been fulfilled or not. The first one is true and the sixth one is true, and if we go back to our dataset, the first one is true and the sixth one is true. There's a way of identifying our parast. Now, let's put this up here she wants me to do something different. What I could do is just take everything that started with paracet and then replace it with the correct spelling of paracetamol. That's probably what I would do. I know there's capitalization as well. I would do a two lower case for all of these strings as well as trimming white space at each at the beginning and the end of the string, just as a general starting point for any of these string manipulations. Why do you just tell me what you want me to do say that so we don't get bogged down in mine? No, I think that that kind of solution is exactly right. So if you just do a mutate and then do exactly what you're saying, like a case win or FL. They'll do FL. I don't use titvers FL with the This works as What's the variable name medicate. I'm using mutate because I want to either make a new column or change a column, make a new variable change a variable, so I'm mutating the data frame, piping in. I want to change medication one. I've got FL, and I'm going to put in my condition here. Into the condition of FLS, but I'm going to take out my old style data frame and dollar because that's all automatically passed to this. I think stop if this is wrong. This is going to give me by true false is true falses. Then the second argument in FLS is what to do if it's true, if it's true, I want to say ParacetAmol, if it's false, I just want to pass back to me medication one. We'll put that into data frame five. I haven't got an error. I bring up data frame five. What I'm hoping now is all of the paracetamols are correct. There may be because I have This regular expression will apply case. This is only going to select the paracetamols that are starting with a capital P. It's not going to select out the paracetamols that start with a small p. If I had this is still wrong because as a factor, this is going to be seen as a different thing. I may want to do a two or something like that first. Right. That's perfect. I think that is This is a vaguely simple spelling mistake, but it just goes to show how complicated. It can become quite quickly. Like an says, with regular expressions, it's usually a lot of frantic googling on remembering the logic that you can use. It can be hard to encompass lots of different mistakes and all the possibilities of mistakes that people could use. Instead of doing something like regular expressions. We sometimes talk about lookup tables, which is also maybe another option in terms of if it's not a spelling mistake, but goes by a different drug name for example. You could use a lookup table and switch things out that way. But instead of doing what we're going to now talk about is or unless we're going to fix this error for. Let's that to talk about what we came here for is a bit of fuzzy matching. We're going to start by Using these functions called stringdist, stringdistmatrix and stringsim It's all about calculating a distance score between two strings. And This is a way of looking into the differences between two strings based on additions, deletions, and substitutions and those kind of things. That's how R is going to be taking two strings and calculating how different they are. It's important to note that in these methods, it's not going to understand, for instance, the synonyms of a word or how similar. We know the words to be it'll basically just look at the the differences in terms of the actual characters. For example, the word hard and herd, as in herd, will be more similar than hard and harder despite those two being much more similar words. Just because it takes one substitution to become hard, but two additions to become harder. Sorry for taking long to do that. That's all right. Okay. So, what we're going to have a go at we'll start with the string first, which basically what we're going to have to take. We're going to want to grab the name column from both of the data frames. I was doing the old fashioned dollar sign grab of the con. But yet having a look through the Help is definitely useful. And is it medications or going to names, sorry. So in your own research, it might be more likely to be matching on different things, but we're here, the spelling mistakes and the not quite exact matches are within the names. There we go. That was nice and simple and the output there, you can see that it's for each of the match of the names within those two columns. It's given you a different score. So zero is an exact match all the way up to well, however many characters are within the name. Say that John Doe and John Doe exact James Smith and James Smith were exact. But Michael Johnson was slightly different in the switch around of the A. Yeah. Now we can try the same thing, but if we try a matrix instead. This example, it's taking the first one and matching it with the first one in the other data frame. But if we want to look at the distance score between all of the different options, you can do the exact same thing but use the function stirngdistmatrix There is a bit more of a complicated output, but you'll see it's for each of the data frames we've got. So the best match in the middle. The two data frames that I made happened to have essentially the match in the same order, but they wouldn't necessarily be that way. This is a way that you could. That's just another way that you could get a matrix to have a check of that. The third one that I gave the string similarity, it's very similar to the string distance, but it gives you a score on 0-1 score. With this time, one being the perfect match. I'm just looking at there's obviously different methods that can be used for the matching. OSA is the default and the one that people can get taught is the Jaro, whats the second guy's name. Winkler, is it? Which is how many letters are the same, how many let many transpositions are there that are different and then a little equation that changes them on a scale of zero to one to give a match. Yes. Yeah, you can change in the method. You can change the different one to what you might want. Yeah, exactly. You might want to use the one you're more familiar with than just the default because that's not always what you want to do. Perfect. Now we've calculated distance scores, we can see how similar our words are. But what we might actually want to do is to join our data frames. Now, I'm going to caveat by saying, you have to be very careful to do fuzzy matching, fuzzy joining. It's not something that you might want to do on a large scale if you can't actually go back and double check things. But for the sake of completeness of this demo, I thought it'd be nice to show you some pretty simple functions that you can use to join two data frames based on not exact matches. We're going to be using this package, which is called fuzzy join and it uses similar syntax to inter join folder and left join, like they do in the tiny verse except it is using string dis at the start. If we have a goal at joining the two data frames based on the t we go, use that one. And use strings join instead. Is the by syntax the same? I think so. We of going over my I'll get rid of this just. Joining by name anyway seemed to work last time. So you'll see on the Help tab that there was a default for the maximum distance. That's set at two. If there is any differences that were lower than two, it wouldn't join based on that. You can modify that to whatever you want. That's why some of the variables still were NAs because the algorithm didn't match them closely enough to match them within this specification that we put. We can make it higher or lower. It's all about finding a happy medium so that you don't match things that are too distantly related and that it can there can be quite a lot of overlaps eventually. But also, you want to make sure that it encompasses enough of your variations there. Michael Johnson and Michael Johnson have been matched because let's just go back to this. Here's the distances. This is a distance of one, but this is a distance of three, and we've set a default distance of two, it'll match anything which has got a distance of one and not match anything that's a distance over one, which is why we see row three, that is now matched, but row five has not matched. As always with these things, I've never done this before. But if we now go to Max dist equals three, then Maybe it has to be over. That has that changed? Interesting. Brian is still not coming in. Dunno? Could it be the method that it's using? Probably the method will be different to the Because you made this one so big, the distance between John Doe and Jane Smith or John Doe and Mr. Jackson is enough that has joined them and given you extra entries within your data set. So that's why you have to be careful with how much of a distance you allow in your join. So that so or at that time. Brian is now in. But David Anderson so line nine. David Anderson. All the letters are different because they're all, well, They're all capitals, one, two, three, so that's the distance of 11 cause they're all caps. So in this one, this example, there is you can ignore underscore case within the strings join and by default. Yeah, if you change that to true, that will know ignore the case. It's still different. Interesting. That's very interesting because ignoring the case, it should be an exact match. It's got David Anderson that time. Maybe I didn't do it right. Let me just make sure the by term is correct. Okay, so that's that's back to three. It's not matching Brian Jones Until we get to six. Yes Bizarre. I'm not quite sure, especially because now we've made sure to match the method that we were using. Okay. But anyway, that is I guess, an example of why fuzzy matching can be quite complicated and quite tricky, and you have to make sure you are aware of what you're doing. Things like data wrangling, data cleaning, probably the most important things to do at the beginning so that you limit how much Fuzzy matching you need to do. But it's always good to know that these things are possible and an option and can be quite useful things like regular expressions can get quite complicated, quite fast. Similarity scores can give you useful insights into your data and can help inform decisions that you make going forward. Yeah, that's pretty much all I had planned unless anyone has any question about fuzzy matching, In one of our COVID projects, we matched the drugs of 350,000 patients against BNF standard drugs using a fuzzy join. The drugs have been typed in by research nurses. There was lots of mistakes. We picked a threshold and just said if it's close enough to the BNF standard using a lookup table. Then call it that. It took it took maybe three or four days to run that, running on a server, running away, but worked very well in terms of improving the matching of the particular drugs with it was something like a 1% false positive and you're just trading off the true positive versus false positive. I think that thing in terms of matching on drugs and maybe like hospital names or things like that are more likely to come up in people's research. Definitely, but it can be computationally. There's a question coming up saying, you ever use longest not sure of the difference between that and string. I'm not sure about you. Do you want to unmute and speak about it claire. Or you maybe can't. No mic. No mics. Well, I mean, stringdist is just a way of applying a number of different matching algorithms or distance methods to a string, and there are different distance methods that start very simply and then get much more complicated. The Jaro algorithm, jaro which is the jW one, as I say, just counts letters that are different and counts up transpositions. That's why I was looking at this because for this, it would say Brian is all correct. Then I think it would count up one, two, three, four, five, six, seven, as wrong. And not consider them transpositions, but it depends on how transpositions are usually just one letter apart. So that would count that up as seven, and then the The winkler part of it was to emphasise the start of the word. If there was a prefix match, then that upped the matching score because a lot of words people get right at the start and then get wrong at the end. I tried to wait earlier letters in the word compared with late letters in the word. String just provides a number of different ways of capturing the difference between two strings, and in different situations you might want to capture them in different ways depending on what the task you were trying to do. I'm guessing longest common substring clear is looking at two strings and then looking for the well I'm just rearranging the words of what you've said, but but capturing the longest common subset. For paracetamol with that error at the end, Paracet would be the longest substring, which would be seven characters log. I don't know if that I don't know if that is option. Do you know if that's an option in this? Yeah, the metric it talks three all the different options. Yeah, there you go. Longest common substr. We we do that? That looks like it's given us a perfect match. You just have to watch these, don't you because this might be. You just need to watch the units of these the longest common substring is defined as the longest string that can be obtained by pairing characters from A and B while keeping the order of the characters in. That's what we said. The LCS distance is defined as the number of characters. The distance is one minus the match on a scale of 01. Okay. Yeah, looks good. Always really good to think about the different options. Because the simpler one while, sometimes easier to understand, might not be the best option for your da. Yeah. Square is also saying sound to create phonetic variations can be useful. That sounds really good. Interesting to know whether that worked for names and that names are sometimes pronounced differently. But there's a perfect match on that as well. With no false positives. You know there's false positives if you get multiple if you get multiple rows. If this is more than 20, you've matched more than one from dataframe 1 to dataframe 2. Great. Brilliant. Okay. I was a really useful point for discussion. Thank you, Claire. I will stop the recording now, but thanks everyone for listening in to our fuzzy matching.