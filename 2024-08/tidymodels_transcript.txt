There we go. That should be us recording now. Yeah. Okay. Hi, Everyone. Hope you all had a really nice summer. Thanks for sticking with us over a little bit of time off. Welcome back to another healthy Demo, today, we're going to be talking about Tidy models and a bit about the tidy models framework and the work flow that comes with that. Today, we've got Ewen Harrison, who is currently sharing his screen. We've got the posit Cloud link in the chat, but recommend following along on Zoom. All of the code that we type will be available on our Git Hub along with our recorded videos after the clinic and demo ends. Yes. We'll just get started. We're going to do 30 minutes of demo and then we'll have time for any of our clinic questions afterwards. We're recording this demo, but we won't be recording the questions. So if anyone has a question afterwards, we'll look forward to those, but we'll just get started with the demo. So, if in the files tab, if you could open up the quarto document that's called Tidy models, I think Yep, right there at the bottom. Is that big enough, Sarah? It looks it looks pretty big on my screen. Neil's put his thumb up Perfect. For anyone that hasn't before seen a quarto document a quarto document is very similar to an R markdown document, which runs with some pretext and then the grade out area with chunks where you can type re code. It's useful for having a document that flows quite nicely. The difference to an R markdown document is the quarto is a bit of a multi engine tool, so you can write Python code as well as R code. So that's what we're going to use today. If anyone wants more information on Quarto, we've done a demo previously, and you're welcome to go back and watch those videos. Perfect. So if you would just be able to scroll down and run the first chunk for us, it's just a bit of loading the packages, loading the data set, and doing a little bit of data manipulation to get things ready for us. A quarto document, you can see it either in markdown format, hitting source or as it parses with visual. This is a chunk. It's R, as dictated here, and I'm going to hit this little triangle to run the code within this trunk, which is opening the tidy verse group of packages, the tidy module group of packages. Bringing in csv dataset, which is called diabetes binary health indicators, but only retaining a sample of that. 5,000 rows into an object called Diabetes. Then doing some manipulation of that with mutate to change variables and we're changing across like the newish way of manipulating within the tidy verse. We're manipulating each of these variables in exactly the same way. And that is to convert them all to factors. We're saying each of these should be converted to a factor, and then we're adding some extra variables. One called added equals one, one called subject ID, which is a vector of one to the length of the dataset, so just one up to presumably 5,000 Then a new variable called diabetes be binary, which uses an if else type argument, but it is an if_else argument where you have a number of different conditions rather than a standard if else argument, which is just just two conditions. If this then do that. But here we're seeing call and a If a variable called diabetes binary up here is the same as one, then change that to yes. If this diabetes binary variable is the same as zero, change that to no, and if there is no one or zero, then make that missing and change that factor. The reference level on the factor is the yes. A very useful way of breaking down what was happening there. If you just scroll up slightly, we can have a quick look at what packages come with tidy models. ID models like the Tidyverse is just a collection of different packages that is useful for the modelling process. It follows the similar syntax that the tidy verse uses in that it's quite intuitive. It is quite human readable and also it's very useful for it produces data frames, which are then easy to manipulate afterwards. They're very useful in that sense. I'm not quite sure why it's taking it doesn't seem to be loading in the environment. Slow. Okay. Cool. But yes, these are the packages that come with it. You might recognise some in parsnip for modelling, R samples for resampling recipes for preprocessing. It also loads all of these other packages like dplyrr and readr, but they also come with the tidy verse that we previously loaded, so they're coming up as a conflict, but it's not an issue. It's just that it's part of both of the tidyverse and the tidy models. This output here is the glimpse. I was just to have a quick look at what the variable formats are. Cool. I guess we'll think if we can move on even though it's not quite loaded. The first thing you might want to do it depends what kind of modelling you're doing. If you doing more exploratory or like trying to do predictive modelling. But if we're trying to make any kind of prediction or inferential modelling, you'll likely want to split your data into a training set and a test set. tidy models has got some really useful functionality for being able to do this really quickly. I've taped out what the code is here, and it's using this function called initial split and then training and then testing, these functions allow you to split your data into the proportion that you initially set up in your initial split in your sorry, some initial split function. If you run that chunk there for a few. There we go. And you can see that in your environment there, you've got your test set, your training set, and they are split up with an 80%, 20%. You can change that with the proportion value there depending on what you want, depending on how many roles you have in your data set. You can split it however you like. We've gone with 0.8. And Another useful thing that you can do with the initial split is that you can stratify with a certain variable. This can be useful for making sure that your variable of interest is split equally between your training and your test set so that you don't all fall into your training. Perfect. So just like wan has done, he's added diabetes binary, which is our variable of interest in the strata argument. And that means that within our two datasets, that there should be equal or proportional number of events in each of those data sets. I don't know if you said this data because I was trying to duplicate things if the cloud wasn't going to work properly. But for lots of machine learning methods, we like to split a training and testing set. We like to hold back a proportion of the data so that we can train on a set of the data and then determine how successful that model is on a further internal validation before ideally looking at an external data set to determine how accurate the model is. So this is holding back 20% of the data, which will not be used in creating the model so that we can determine using previously unseen data exactly how successful our model building has been done. I guess we'll do that later on. As always, I haven't seen this before I don't know anything about it. Now, if we have this 80 and 20% split, we probably want our outcome to be equally distributed between these. Diabetes binary is our outcome here. If we were randomly sampling, then we may just by chance, end up with a different proportions of our outcome in the training and the testing set, which may cause problems. This is a way to ensure that we have an equal number of the outcome across the testing and the training. What sometimes to do here is to set the seed. That sets the random sampling, and if you do that, set s then split, then every time you do that, you will get exactly the same split as you on repeated occasions. If you don't have that there, then every time you hit that line, you will get a different split, and therefore potentially different results. So if you want things to be consistent between times that you run it or if you're sharing it with other people, then if you set the seed, you will consistently get the same output each time. Yeah, just to reiterate that there, yeah. Setting the seed is definitely important when you're doing this kind of training and testing split, especially for reproducibility. Perfect. Yes. Okay. So once you've split your data, the next thing that you might want to be doing in your tiny models workflow is creating a recipe. A recipe is basically just a set of instructions that you are giving for pre processing for things that you want to happen to your dataset before you jump into fitting your model into it. So Well, you start by explicitly stating what your recipe is, which is basically just the formula that you're going to be running. Here basically just means everything. It means all other variables, which you could use what you might be more familiar with is just or what we typically teach is using the naming all of the variables with a plus in between. But using this dot just means every other variable that I've got in my dataset. And then seeing the dataset that you want to be working with. So in this instance, we're always going to be working with our training dataset to begin with. We don't want to be looking at our testing dataset until we're finished. We don't want to be changing our results or changing our ideas based on how the testing outcome is. Throughout this document, I've added a few little helpful links that all mostly go back to the tidy models website because they've got some really good documentation there. It's really really useful. I definitely recommend going to check out these websites, especially if you're just getting started with tidy models. Basically on that website that's there, it gives you a big summary of all the possible things that you might want to be doing with your recipe. You can do loads of things you can do normalising, for instance, here, I'm going to get us to remove the columns, the zero variants, add some dummy variables. And there. But yeah, as an showing, you can search there. And really, especially now, the Tidy models team have really been developing loads and loads and loads of steps that you can do. I'm pretty sure there's one where you can it's like step holiday where if you've got a date variable, it can tell you that lands on Christmas and things like that. Which might or might not be useful in an analysis that you're doing. But really, there's so many options for things that you might want to do when you're pre processing. Cool. And what I was going to get you to do is I've added in the step zero variance, which removes any columns that don't that are consistently the same. I was hoping you could add a column that would create dummy variables for us on our M nominal predictors. Great. This is a really useful principle, this recipes business. It means that you are providing instructions for how the data needs to be preprocessed, but without actually having to do that in time. It means if you're creating models in a number of different frameworks, maybe some machine learning models like random forests, and maybe a traditional logistic regression and maybe another approach. Then you can prepare the data so that it is so that it is ready for all of these different approaches. Dealing with missing data, for instance, dealing with imputation, dealing with zero variance, as zero variance just means that the number is the same throughout that there's no difference. And dummy variables. Not to go into this in detail, but if you're doing a regression analysis on a factor, then under the hood, the model is usually creating a dummy variable. That, for instance, creates for a three level factor, two different variables in order to describe that as zeros and ones. Zero, zero, zero, one, one, 011, et cetera across two different variables. But you can do that all automatically through this. For things like base R logistic regression, it's done automatically and you don't even know that it's being done for other types of machine learning model, for instance, like gradient boosted trees, you need to do that before you pass it to the model. The model expects to receive what gets called, and I'm sorry, if this is too detailed, but a design matrix. That is a matrix of all the Xs on the side on the side of your equation. So what we've got here, we've got an object called diabetes recipe. This formula here is really just telling recipes how we want to specify the variables. We're telling it that diabetes binary is an outcome measure and that everything else explanatory or independent variables. That doesn't change them at all, but allows us to specify within the arguments that follow, whether we want to do something specifically to the outcome, whether we want to do something explicitly to the independent variables, whether we want to do something to factors, something different to a continuous data, we might want to normalise continuous data. Centre it over zero and make each step a standard deviation, but you wouldn't want to do that to a factor, you can do that separately. If I just run this, just do that like that, then that gives me an object, which is a recipe and it tells you what the inputs are. It says there's one outcome and 23 predictors, and that's all that that does. Then if we do the next line, the step, then it gives us that same information, but then has told us that it's done an operation zero variance filter. Although it seems that that hasn't applied to anything. I'm not sure if that's right. We want to do something else and we may go onto the Internet and find things that we want to do. S is told us that we want to make a dummy variable. This is the step that is required if we want to do something like a regression and Step dummy is what we use for that and it is going to apply it by default to the predictors only, not to the predicted, there's various different options that we can do for that. But let's just do step dummy. Then if I run that, I now have two operations that have been done, zero variance filter and dummy variables from. Why does that say it's not being done in anything to ero? Yeah, it's a good question because it definitely should also be removing the zero variance. I think it's within the brackets of each of those things. I think it might help if we do a bit of a selector and say for all predictors. Although it should be doing it like you said on the Help tab. But often I'll I'll say all predictors or all nominal predictors or which ones you want it to apply to. Maybe it's doing it, but it's just because we haven't specified. I think I think that none just means we haven't specified what it's been done on. Does that make sense? Yeah. One more thing just in our recipe. As you said, the formula at the top in the recipe is saying what we want to work on, and the dot is very useful if you want to apply it to all of your other variables. But there might be a few variables that you don't want to be included as a predictor. In this example, we have a subject ID column, which is quite common and you probably don't want that to be thought of as an explanatory variable. So one useful thing I often use is this update role function, which is, that's That would be one way of doing it. Yeah. Within your recipe, you can Yes. You can specify different ways of removing and adding variables but date role is a useful one for basically, you'll want to tell at the column that you want to look at. And in this case, we're going to, exactly that. ID. Yes. The new rule can be anything. It can be free text. It's usually just a useful way to break up the variables that you're using, and if it's not predictor, that won't be thought of as a predictor variable. It's thought of as an ID variable. If you have multiple outcomes in your dataset, you might want to update the role of those outcomes so that then they are not put into your model as a predictor. They'll be kept separate. That can be useful for then using this dot argument, which means all of my things that are predictors, essentially. Perfect. Yes. That is a bit of a wisp of recipes. You can just in passing. This hasn't done anything to the data, but you can add pre. I mean, not that you would do this in this workflow, and that actually That actually processes the data and tells you what has happened with the processing. But that's why this is now changed, trained. This is donees to the data. And then the final one that you can add is juice. They're all of recipe type and that will actually give you the set. That has now done everything that we've asked to the data and passed us back the process dataset. So we would be able to see that dummy variables have been created. So these are now dummy variables for i BP and high cholesterol which are now os and ones. Yes, brilliant. Yeah, it's all very recipe based. You can also use bake if you want to apply your recipe to a different dataset, but in the Lingo. Cool. The next thing after you've made your recipe is you'll want to specify the model that you are wanting to work with. Again, in this link that's provided in this document, it takes you to Tidy Models documentation, which will show you all the different options of models that you can work with in the Tidy Models framework. There's lots. There's many you'll think of. I was recently at the posit conference and they were talking about how they've just recently added survival analysis as well as an option within the Tidy models framework, which is very cool. Yeah, this website here is really useful for finding out all of the different models that you can do, and then it also tells you the mode that you'll be working with and then also the engine, which are things that you'll need to specify within the specification set that you're doing in your code. So parsnip is a way of using exactly the same syntax to run lots of different models from lots of different packages. If anyone has done this thing before, this is the base R logistic regression. This is what you would run if you were running GLM with with a formula. Family equals binomial data equals data, et cetera. But this is how you can run it within this parsnip or this tidy models framework. The advantage being that if you wanted them to swap out BaseR GLM to a multi level general estimating equations or glmer, which comes from LME four or I guess this is another package that uses it, then you can easily just change the underlying model without having to change all of the syntax. So that's the advantage of it. Yes, exactly that. I think one of yeah, the main advantage of tidy models is that kind of flexibility in being able to kind of quickly whip through a bunch of different models and test. Yes. I rule out the syntax for using a decision tree and I was wondering if you would be able to use that same syntax, but maybe create either talk us through this or create a logistic regression specification. And We're trying to do some classification. We're trying to determine the probability of a particular case, I think, patients as to their probability of having diabetes, yes or no. We can do that in lots of different ways and logistic regression is maybe a traditional way of doing that and a decision tree is a different approach to doing that. Again, this is setting up a set of parameters for a model without actually doing anything with it. If we run a decision tree here, then that tells us that we're doing a decision tree, it says unknown mode because we haven't set that yet. Then this cost complexity parameter, which is part of decision tree modelling is mentioned there. It must know that the default engine for doing this is our part. But we can set the engine specifically, and then we can set that we're interested in a classification problem rather than a regression problem. This sets up the sets up the modelling that we're going to do. S has asked us to just quickly change that to logistic regression. Change the name here to Log g. I really don't know how to do this genuinely. Let's go to logistic regression. You're on the recipes at the moment. Thank you. Maybe if anyone's get questions, just if anyone wants to ask a question, they could maybe put it in the chat just so we know if we need to stop because I'm taking longer than Saba has intended here, so I apologise. What I'm going to do is, I'm just going to use Base R GLM. I was saying before, this is Base R GLM. This is exactly the same as you would do normally. If you use final fit, this is exactly the same under the hood and final fit. And here's the model specification here. I'm going to just copy that, move over here and change this to logistic region with some brackets. If I run that, then it's going to now tell me that I've got a logistic region model, which again, actually it's defaulting to classification and it's defaulting to GLM. But we could specify that specifically as GLM and classification, and then if we do that. We've now swapped out our decision t model with a logistic regression model, without asking him to change anything in the data, without him to change anything. We could now run these two models separately, compare them, do they work well, one better than the other, et cetera. You can see that if you have a modelling problem and you're not sure how best to tackle it, you could very quickly set up five or six different approaches to doing this to try and find out which one might be best. Okay. Brilliant. Yes. That's exactly correct. And yes, so it's kind of defaulted to what we added in with the set engine set mode. Theoretically, you wouldn't actually need to do those if you were doing this default, but it's quite useful to get into the habit of using those particularly if you start doing different modelling that isn't necessarily going to use the default. F1 on these, which brings up the help page and the help page, usage has the default. F1 and logistic regression takes the help page, go down to here and this tells you what the defaults are. That is why when I run a logistic regression just by itself, it knows that it's classification and knows that it's GLM because that's what specified in the underlying function. Brilliant. Yes. And so now we're on to workflows, which is a really useful package and function that's part of the tidy models workflow, which is basically useful for what you and was talking about earlier in terms of being able to quickly switch out specification you, which kind of model you're running so that you can compare and contrast different results. So basically workflow is just a way to bring all together the recipe that you've created and the model that you are going to want to run, and then you can fit it on that. I've pre typed out the syntax that you'll need with using an underline where you need to add in the things that you are wanting to work with. We need to add the recipe that we've made and the specification that we are choosing. If you want to add our recipe and then the logistic regression model. This now brings together everything that we've done. I just run this workflow function, then it'll tell me it's workflow, but there's no pre processing. That means the recipe, we haven't specified that, and there's no model. We haven't specified that. Let's go back and add our recipe. Here's the diabetes recipe. I'm just going to copy this copy and put that in here. Now if I run just workflow and add recipe, I will have can add a recipe to flow. I think that's maybe let me just run that again. The reason I did that was because when I was demonstrating the prep and the juice business, I had got rid of the recipe. All I've gone back is run and make sure that it actually is a recipe and then copied it down, and now we've got workflow, pre processing recipe, pre processor, and then it's telling us what the recipe steps are. Now we can add the mo, so we're going to go back and we'll take the decision. Diabetes let me just rerun this to make sure that I've done it correctly. I'll take that and add model. Now I'll just run those three arguments. I keep missing out that work. Here we've got work flow, pre processor recipe, model decision, the pre processor steps, the model, that's brought it all together. Now we can define a workflow, which takes together the dataset the recipe that we want to use together with the model that we want to use. And again, so you can imagine setting up lots of different datasets and models in order to be able to do this. I was doing this one and one of the models that we was trying to fit was Pas regression models. A Pau regression model is one in where the outcome is typically a count and what was the thing? I can't be zero. I was having to change the dependent variable specifically or it can be zero. I remember what the difference was. But I was having to change the underlying data specifically for this pass model. I was able to do a different recipe specifically for the PSO model. That's how it can bring different datasets for different models together. Sarah. Brain. Yeah, once you've defined your workflow there, the last thing to do is to fit your model. All you have to do is give it the workflow and then give it the dataset that you're wanting to run it on, so that will be our training data. Yeah. I at. I want to know the I want to know the arguments here, C. Yeah. No a particularly useful help tab there. I kind of the same as this, isn't it? Well, I could I could. So how do I specify the work flow? Sorry. Let me do this. So let me go back to tailing models. I'm going to put to Parson because that's where we are. Reference. This is what I'm trying to get to. So Object formula, case weights. A object is a class of model specification. So I'm presuming then that I can put my workflow and a that. Now if I do that. That's what I get. Yes. That, that is perfect. And it's also really useful to see the way you go about finding your help resources and finding the solution. So, how it should look after fitting the kind of decision tree. Model. I'm just trying to without data because we have specified data in the recipe, but that doesn't include data in the recipe, the dataset is not included within the recipe. The recipe is only a set of instructions, which provide the data set at the step. You'll see that that's important when we're swapping in and out training versus testing data set. Exactly. Yes. Because we're kind of running low in time, I guess, we'll quickly jump to ways that you can pull out evaluation metrics from this model that you have just fit. Just swap in and out this just so that can maybe see a clearer clear outbreak because because the decision tree output is a little bit hard to interpret. Yeah, it's So new work flow that I'm calling the log ridge work flow. That error there because I hadn't actually run and made this object. I'd just been running it like this for demonstration. So we have a new work flow. Make this a new line. I'll put this in here. In again in the future, I'll put this in here. You've got a couple of extra letters in that penultimate line. Flow. You need the second O. Thank you. Here's the output from this, which is possibly more familiar. So the recipe and the recipe steps, and it's not formative particularly well. I do this. Does that help? There's a standard output that you would get from a base R regression function with the call to the function, the intercept, because this is a logistic regression, these logo. The exponential of these is the odds ratio for each of the variables. Okay you say that to you? Yes. Yeah, I agree. The logistic regression is much more familiar to most people. So that was actually really helpful. From there, if you want that output or table, you could use various like GT or cable, but we'll skip on from that and we'll pull out some beta metrics from our prediction model. Here is using augment to basically predict on the data that we're using. And So, U and has just run that and is showing us the output of basically each of the predictions and retaining our original data frame as well. So this prediction class, is no yes for each individual patient. So this is a role and The first patient is predicted to be a because the predicted probability of yes is 43%. The second patients predicted to be a, the third patients predicted to be a, the fourth patients predicted to be a yes. That's because the default here is a 50% cut off. On a predicted probability above 50%, it's saying that the predicted class of having diabetes not diabetes is yes. For those that have a predicted class of under 50 is no, and you can change that to you wanted. And here is the actual classification, the observed variable in the data together with each of the other variables that have contributed to the model. Brilliant. Yes. So if we wanted to pull out an AUC value, for example, what we would do from there is we would use this function that I've added just in the text there, Rock underscore AUC. So, right. Yeah. And You'll want to take it from that new dataset. So you'll want to take it from Augment or if you pipe the augment into the rock AUC, or if you say that is a perfect. Then what you want to give it is what the truth is or if you did a help tab, you'll probably tell you what you need. But we basically want to give it the truth and then the prediction. The truth then the prediction. So just going back to augment the truth is diabetes primary. Hope that will be o that type of variable. And and the class. Yeah. Sorry, this is annoying. What we're trying to do is we're trying to determine the area under the receiver operator curve, which is a commonly and sometimes maligned way of determining the discrimination of a prediction model. So 0.5 is the toss of a coin and one is perfect prediction where the model successfully classifies every example within that set as incorrect. And a good model would be upwards of 0.8 on area under the receiver robert curve, and moderate would be 0.7 0.8, I guess and not particularly useful less than 0.7. Now I was giving us an error here because it's say estimate, which is it should be a numeric variable, not a factor. So that's in reference to you giving the class. Yeah. So what should I be giving it? In this instance, you want to give it the prediction probability of it being a yes that. Yeah. So it needs Here we are this prediction model on the trained data. We don't really want to estimate this on the data that we've used to train it because if the model is working well, then you'd expect that to be good. But it is 0.80 0.82. C is These are just other ones for creating a curve, so you want to do it off of that right does this need to be? Yes. No, you need curve and then you need it to be the diabetes binary. And the class and then you need to do auto plot. If I do that, first of all, that will give me all the numbers for the plot. You've done this before you'll recognise this plot. There's the classical area under the curve rock plot, which is one minus specificity on the x axis and sensitivity on the y axis, and the optimal point is where this line is greatest between 45 degrees and here. If we were to set a cut off say I 77% specificity and 7% sensitivity, then that would give us our discriminating model. Which would be the one that would be best to classify Is this the same with this? Yeah, but you need to give it the class instead of The s. This is going to be a confusion matrix, which the two by two table. And it will default to a 50% cutoff. And I need the class a That gets called a confusion matrix where this is the model prediction, and this is the truth. Yes, yes. This is the model prediction, no, and this is the truth, no. The diagonals are the correctly the true positives, true negatives, and then the false positives and the false negatives of diagonals. We could change and presuming augment what the cut off was for the classification of yes or no to a different proportion if we wish. Yes, brilliant. These are just some quick and easy evaluation metrics that you can pull out. There's various other ones that you can pull out Brier score, accuracy, so on, so on. Basically just different functions to specify how you do it, and it depends on what you're trying to pull out. But this is pulling out AUC, which is useful sometimes depending on what you want. The last thing that we're going to do quickly is we're assuming that we have put in lots of work in our model, and this is exactly what we want. So we want to now see if our results are really strong results, go mirror from our training set, but also in the withheld test set that we created previously. This is data that the model hasn't seen before. Is that right? To be honest, I would usually just do it within last fit. I would just give it the work like last fit, the work flow, sorry. I've confused things. So you don't want to give it the test set. You want to give it the split that we've created. Yeah. This is where we defined our training and test splits. Can I just do that? That is expected? No. If you try I'm not sure why it's different. But if you try doing not without the pipe, if you try putting the work flow within the. I do any I don't think H. Oh, wait, sorry. It's because we've not collected the metrics. That is exactly what it was meant to be. That was me being silly, sorry. Yes. That's pulling out the metrics, the accuracy, the bioclass, and the AUC from the test set. It's exactly the same actually. This is taking our workflow then and using the training dataset to train to process the recipe to run the model. But then the model is tested on the test data set, which is also subjected to the same recipe. That's the advantage of this. The test data set is treated in exactly the same way as the training data set. Then these model evaluation metrics are on the withheld en test dataset. This area under the receiver operator curve is very close, but 0.8 to 4165, where is above we had 0.8 to 84494. The mode is running just as well in the unseen data as it is in the data that has been seen. As you go on and do this in more detail, you might you might use sample to sample different splits of the data in order to understand that in greater detail. Brilliant. Yes. Sorry, that took up the entire time, everyone. As you alluded to, there's lots of different ways for that you can go further and more complex with tidy models. There's resampling, which is very, very useful. Likely something that people will want to do, but there's also tuning, et cetera. It's definitely worth looking into if it's of interest, we can do We can do another tidy models demo at some point. I think probably managing re sampling would be useful because that's a good state. Yeah, for sure, it's definitely one not to miss. Sorry. I feel like that was a lot of information to taken. It's a very versatile, useful tool, but it can be a lot. But once you get used to the syntax and the flow of things, it's very, very easy to use and to be switching around for different model evaluations and things like that. Sorry to have taken up the full hour. Next time, we'll definitely save some time for questions if people have any. Yes. The next demo is in three weeks time. Feel free to Come along. Share with your friends, family. And, thanks, everyone. If you hate your family. Invite your worst enemies. Nice to see everybody. Cheers. Bye, the dog.