There we go. That's us rolling now there. Hi, everyone. Welcome back to another Healthyr Demo. Thanks for coming. Today, we're going to be covering the package Tidy text, which is a package that can be used for text mining and various other things. At the top of this document here, I've got a link to Julia Sg. She has created a training app, but she's also got lots and lots of documentation about tidy text, and I 100% recommend going there and reading her book and looking at the resources she has. They're really really helpful and can highlight a lot of different examples and applications that you can use tiny text in. But for today, we're just going to jump into this demo. So as all we've got we're using Posit Cloud. The link is in the chat, if you want to follow on along, but it's recommended to be following Ewen Harrison's Zoom screen you can currently see sharing. Yeah, we're going to start by loading the packages we want. Today, I have decided to lean into one of my own interests and not quite as applicable to healthcare data as usual, but that being said, we've looked at aliens before. We're going to be looking at Taylor Swift lyrics, and we're going to have a bit of a breakdown of those. Yes, we've got the Taylor Swift lyrics are coming from a package called Taylor. And yeah, you and if you just think started with the packages and data. I can't believe we're doing Taylor Swift, just for me, As as a massive Swiftie. Thank you. We're in pose or our studio. We have a quarter document open here. You can flick between the two different ways of visualising a quarter document, source or visual. To be honest, I prefer source more than visual just because it's what I'm used to working in, so I'm just going to leave it like this if that's okay. We've got our first chunk here which loads three packages and control entering down these three packages, Library Taylor, who knew. We've taken something called Taylor album songs, which is an object loaded by the Taylor package and from right to left assignment, renamed that as T Swift, which now appears in our environment as data frame or tipo with 29 columns and 240 rows. Brilliant. Perfect. The first thing I want to get you to do before we jump into tidy text is I've just left a little space there for you to have a quick look at the data to see what we're looking at. You can either type code or you can just explore it in different ways in the environment, whatever way you would prefer. Yeah. I'd do three things. I'd click this arrow first in the environment to have a look at the at the data. We've got album name, EP, album release, et cetera. It tells you what the type of column, character, logical date, integer, and the first few lines. You can skim down this and get an idea. Acousticns is sounding very interesting on a scale of I'm guessing zero to one, speechiness, loving it, et cetera. Or a more formal approach. We wrote originally into the final fit package. L et's see if this comes quick. A couple of ways of quickly visualising datasets. Is that there now? One is called missing glimpse. One is called off glimpse. Glimpse. Missing glimpse gives you quite a neat data frame which tells you the variable n, the label of a variable. So how it would appear in a table, the variable type, and the the number, but importantly the missing. Here it's difficult to see the missing this and that can be really important. For instance, fur, which is presumably another artist featuring in a song is only present in 23 of the songs. If we were to include the featuring variable in a regression model, we would only have 23 lines in that regression model. F F glimpse splits the data and it doesn't work. We'll forget that for now. Perfect. Yeah. That's a really good way to quickly have a look at your data set and particularly visges. When you're jumping into analysis is really important because it can really help highlight any kind of bias that your data might be bringing. Yes, brilliant. The next thing I want you to do is you might notice es, exactly that. The the lyrics column variable is actually a bunch of data frame different tables within this data frame. Yeah. What I was going to get you to do quickly before we go into any kind of tidy text examples was just quickly unnesting the dt of frame, which I've done for you in the next chunk just for the sake of convenience. But yes, this unnest function is for splitting up or unnest these tables that are within our main tailor Swift. The FF glimpse works on a flat data frame, and we have data frames within data frames, which is why FF glimpse doesn't work because you can't summarise easily nested data frame just in passing. Here we have Taylor album songs, which is the original name and then being piped into unnest lyrics, and we're re saving that back into this object called Taylor Swift T Swift rather. If we do that, then we can see that the number of rows that we have has jumped up to 12151. If we just go. The other thing I'm just going back, I think I said three things. So the first thing was look at the IRO, the second the third thing I was going to say was to explore the data to click on the link. And just look at the data in a spreadsheet format. If we skip along to here, then we can see that lyrics is nested. We can actually open the lyrics nested data frame in its own data frame to visualise that. This is the lyrics that are in the first role. When we unnest, we duplicate the row data across however many rows are in the nested data. If that makes sense. This nested data for this first row has 55 rows. This single row in our original data frame is duplicated 55 times against each of the lyrics there. If we go back to that, click that, then we can see our first row has been duplicated 55 times down to 55, and then it'll change to a different song to allow each of the rows of the lyrics to be duplicated up. That's what unnest is doing, if that makes sense. Thank you. That was a great explanation. Now we've got our dataset, and as you just showed you, it is of split. It's got the albums and then the songs, bunch of other information, and now we've got lines of lyrics. Now we're jumping into tidy text. What we often would think of is a unit that we're interested in in this instance with lyrics could be word. Right now we've got lines, but we actually want to break it down into just each word of the song as thinking of it as a token per word. Tidy text has this really useful function called ness Token. I was wondering if you could have a go using that to basically make this dataset even longer and have a line per word. Text analysis and natural language processing, and now generative AI use the word token. Token is the smallest unit of analysis. For this demonstration purposes, it's word, but in other contexts, a token might be a letter or might be might be some other piece of information. So I'm presuming that unnest tokens is a function that comes from the tidy text page, a package I'm not really very familiar with. I've copied I've copied the function here into the chunk and then I've hit F one to bring up the help for this function. It shows me the different arguments within the function, table, output, input, token, et cetera. Then I can ski down to look at examples of how it is used. So I don't really know what I'm doing, but I'm going to take my Taylor Swift. I'm going to take Taylor Swift ti, and I'm going to pipe that in just as they've done here into unnest tokens, and I'm going to presume that I do output equals word. I think that will be a column name. You could check up here, output column to be created input column. Then I'm going to do input equals, and my input column is L lyric. We'll just run that without saving it at the moment. That's given us a ti, which is 88,000 long now. If I skip along word is presumably the end, he said the way my blue eyes shined put those. That looks like it's taken lyric and basically split those words across rows. Yes, that. That's exactly what I was looking for. That's perfect. Just while we're talking about this, we're going to go forward with the one word as a token. But if you have a look at the third example down which uses n gramme and n equals two in the Help. Oh, fit your own. If you scroll down to the examples. The one that. This is another way that you for example, if you weren't interested you weren't interested in just the one words as a token. You were actually interested in little chunks as your tokens in this case, two words. Maybe that could provide more meaning or just relevant to your analysis. You can also use this as an example, which breaks your token into 22 word chunks of the lyrics. There's just different ways to do it depending on what you're actually looking at and like you said, depending on what your smallest unit that you're interested in. Brilliant. You've got the data set with the split into words. Would you be able to have a quick look and see which words are the most frequently used in Taylors? That's hard. Why just save this newly as this? We don't have to go back. Yeah, we don't have to go back. A or word frequency. That's commonly first step in these analysis. I've done this once doing a Twitter thing where I put a whole lot of tweets down and then a count that words across things that were interesting and a sentiment analysis, whether the tweets were positive, neutral or negative. But it was literally a ten years ago, so that's the last time I did this. What have I done? I've just gone on to Google, I've searched for the tidy text package. I've gone to this Tidy text help page, which I've found and now I'm just going to search this page for frequency, analysing word and document frequency. It's giving me some chat about what that means. This is a Jane Austin example. Count. Was that. I actually just using a deplier function. That makes sense. Rather than any tidy text specific function, I can just use a standard tidy verse function, which is basically just counting up the number of different instances of like a factor or a character within a particular variable. I should be able to just go T Swift count word, and that will give me each of the words, 05, 116, 16, it's the numbers first. As they have done here, sort was true. What does that give me? Here, this is now ordering by frequency. U is the most common word in Taylor Swift songs, Swifties will not be surprised by that. I closely followed in second place, and to me in I guess, not surprisingly. Interesting. Yes, brilliant. Yes, exactly. Using just tiny verse function. It's not specifically tiny text. One of the useful things about tiny text is it creates data frames that are very tiny verse friendly. A lot of functions you know we'll be able to utilise in the same analysis. The book that you were looking up at is the one of the books that I was referencing to is a really useful one for just the text mining. That inverse word frequency is One of the more complicated, maybe not the right words, but one of the approaches that we're not quite going to reach in this dm, but is a very useful thing to think about if you were doing some analysis. Right. You'll see that a lot of the words are some of the words you might consider them to be not very informative, not that useful for any kind of an analysis. And within tidy text, they have basically got a function which helps you produce a list of what they call what is stop words. These words that aren't as informative in your analysis, and you might not want to keep and in your list of tokens. This might not be the case always, having a set list of words to remove might not be the most appropriate thing for your n. But it is quite a useful starting space for removing words that might are used frequently in this example in the English language, but don't provide much context to your work. What I'm going to get you to do is have a look at this function get stopwords and then also see what approach you would take to remove the stopwords from our table of Taylors fc. I can run this function, get stop words. This function is pulling from a source called snowball, which is, I guess, just like a number of options of where you can get these from in a particular language language English, being English. This is pulled 175 words, which it considers stop words. I, there's going to be no words left tail a swift song I take all these out. Is that an insult? It's simply an observation. How do I take these out? Well, to remove rows from a data frame or a ti. We typically use the function filter, which is which is a tidy verse. I could skip to this and find removing stop words. I guess that will be there. But let me just try doing it myself first. I'll save this first of all, I'll just call this stopwords. This is now a function, is now using the function to create a data frame called stopwords. I'm going to get T swift. And I'm going to filter. And I want to filter words or words rather. In. This will probably give it to me the wrong way stop words. I'm just going to use traditional old coon, which we don't use anymore. Word. Stop words word gives me a vector of those words. I'm basically saying filter word in stopwords. Usually gives it the wrong way round. In doing that, I have a only kept the stop words. I need to reverse that. I can put just a exclamation mark at the start of that, which reverses that round to say, don't keep what is in this, which gets me all of this is now 41,000 rows long said way blue, I shined put Georgia star Shae. That looks correct, is that correct? Yes. Well, that is definitely a correct way of doing it. There's a mu s way of doing it, isn't there? No, I mean, that's perfect. The kind of examples that are often seen online is using the function anti join, which is quite interesting. It's basically joining on everything that's not in data brain B type thing. But you've got the exact answer there. A nice example of how coding can be so creative and that there isn't nesly one correct answer for getting the correct output. Antoine is a join. They've changed this quite recently. We've got to do this now, which is a bit of a pain joined by. I think we've got word in both of them. There you go. Anti join is saying, bring together the data frames where there is no match between two cons rather than a more typical way of joining two data frames where there is a match. Left join, right join join, they're all saying, bring the rows where the variable matches anti join is, bring the rows where the variable doesn't match. You can see that we get exactly the same, just if you look up here, tile, we get exactly the same ti by doing it in these two different ways. Perfect. Like I said, that might not be the way that you want to remove words from your analysis, but it is a nice head start if those words are going to be clogging up things that you've got in your output. So next thing that we're going to be looking at is a bit of a sentiment analysis. We're going to be seeing m words that can be considered positive or words that can be considered negative are elected or sourced from this next text function called get sentiment, and it's quite useful in its summarised a quite complicated analysis of taking a bunch of words and they have been classified as whether or not they're positive or negative words. What I want you to do is in the words that we have our lyrics from our Taylor Swift dataset, if you could join this sentiment column so that we can analyse whether or not Taylor Swift has got a lot of positive or negative words in her lyrics. This is really a simple join task. I'm going to use left join because I want to keep all the rows in the dataset, which is on the left of the join. Sometimes it's easier to see this if you don't use the pipe. There's the left and the right is this and actually, Word and word is the same. I wouldn't have to do anything further. I could just do that and it will down here say joined by word unexpected, many relationship between x and y. Let's have a look at this. It says joined by words. What I sometimes do is now a copy of this back in here, just from laziness. I said warning detected and many to many relationship between x and y. Well, it thinks it might be unexpected, but we think it is expected because if the same words is duplicated by Taylor in her songs against what appears in the sentiments, then you'll get this Many to many relationship, is that right? What do you think said about that warning there? Yeah, I think you're right in that it's saying that the sentiment might be appearing multiple times in our Taylor Swift data set. I Taylor Swift might have used the same word multiple times. Here's the problem. So sentiments for some reason, has envious enviously and enviousness twice. So that might be a problem in the man to man. So NV has appeared in a Taylor Sif song. When it's tried to match, it has found two examples of this. That may be just some problem with that. We could fix it, but we will just ignore it for now. Awesome. If you see that dataset, we now have the sentiments along with the lyrics that we have in the Taylor Swift dataset. The next thing I'd like you to. Taylor is crashed pose. Is that too big or something? It shouldn't be. They here to run everything from the top. A new error that's come point is trying to load the fine. I'm fine it. I. Sorry, not quite sure what's happened there. It might have been that the set is getting quite big. Hopefully it will work this time. Once that is finished loading, the next thing I'm going to ask you to do is, I've said plot to visualise, but it doesn't necess have to be a plot. It can be a table. Basically was just want to see what Taylor Swift's most positive most frequently used positive and negative words are. Then after that, maybe get to group by album. So we can see which albums are more positive and more negative. I think this is maybe a problem with the free posit space which doesn't have very much mp. Yeah, maybe probably. It should work. Well, hopefully, we eventually. In the work space as well, causing problems. We can give it one more try and then if no, I can share my screen instead. That's really good idea. If it's not working, we could just filter to one specific album and then It won't be as large data set. Is the man relationships that you want to share at the moment and then we can switch back. Let me just get rid of the. It's just crashing all the time for me. But maybe it's to many problems. All right. Just lining at the projects. She was go. There we go. Sorry about that. This is going to look slightly different to what UN was doing because I've not got the answers that he typed out. I'm just going to scroll down to the answers that I made, which is doing essentially the same thing, but I will hide that answer, and I'll just load here. There's a grey bar covering the code on your screen? Sorry. That is we should be no. Now it's the big the middle will appear in just a second. I found something interesting. The reason that Vous appears twice is it's down as both a positive and negative sentiment. We need to decide as a group whether V is positive or negative. It's prey negative. I'm going to say so. Have you got some functioning code happening cause mine is also being really slow and not very interesting? No, yes. Seconds. There we go. That seems to work quite quickly. We have a Taylor sentiment there. We can scroll belong to the end and we can see that we have got the word, which is considered a negative word. Here we've got right is positive word. There we go. So what we want to look at next is within this m within this dataset here. We want to what I was wanting to have a look at is to see which words are the most frequently used positive words and which words are the most frequently used negative words. To do that, what I would do would be group b and group by this sentiment column. Then after that, I would do what we were doing previously and do just a simple count and then count the words, and then equals tru just so we can get the so we can get the highest numbers first. You can see here that the most frequently used positive word that Taylor Swift used is the word like, and then it goes on to love, right, good. Negative, bad, so on. This example of the most frequently used word is another example of how things might not be exactly what you want them to be. Like here, you'd be talking about liking something, but it could also be just Taylors using a lot of similis and metaphors. That's something to take into consideration when you are doing your analysis. What the next task we were going to do was Maybe just moon positive and negative. Then, I wanted to do by the album. Yes. Is this big enough? Now we've just got the most positive and negative words. But what we might want to look at is whether or not an album is considered a positive or a negative album. This might be a bit trickier. You probably might want to do on a different scale and album like you might be looking at, you might want to break it down into per song or some other units because often an album can have lots of emotions. But we'll just stick with album not by now because it's a unit that we can look at because there aren't too many albums. We just add that sorry group by sentiment, and then also count the album. Straight. That is not the name of the variable that we want. Let me just double check what it is. So text called album name. There's lots of different things that you might want to do after this, you might want to visualise, you might want to change what you're looking at in terms of if you want to look at per song. Yeah, I don't know if there's anything else that folks have suggestions on what we could do or a question about this that people might want answered, or you and if you had if you wanted to continue on about deciding the sentiment of V. That's great. That's really good. So have you done any other analysis with this package, this taylor package, sir? Not with Taylor so far. I was just trying to find a data set that would have lots of words in it, and Taylor has lots of songs, so lots of words. Maybe it's something I'll look into more in the future. Chloe saying she loves that Shake is there 70 times. It's a negative sentiment here. I think Shake is a very positive sentiment in this context. This is true. Taylor definitely uses it in an empowering way, which would be considered positive. Yeah, just let me. I want to stop sharing for a second so I can see any chats. Yeah. I suppose that is the end of the current demo here. I don't know if anyone has any questions or suggestions of things that we might want to do next or something completely unrelated to this demo. My posit instance was definitely dying there because of memory. I guess that's maybe a reason not to use the free tier on posit for anything other than really basic stuff. Yeah, I can get quite quite fast. So, I can slow things down. So what would be typical things you do when this happens? A part of drawing money at it. Well, good question. The resource that is available here is only aab of memory, and a gigabyte of memory for of things is small now. So all of the analy all Most of what we do in R is done within RAM. Different approaches to computing, we'll use on disc versus in RAM analysis and in its default state, we'll do things in memory. There's a couple of things you could do. You could look for disc based approaches, batching approaches where you take some of it into memory, do something to it, and then put it back onto the disc, or you could look at more more computationally efficient ways of doing that particular task. Tidyverse functions tend to be written for convenience rather than computational efficiency. That left join function that we're using there is almost certainly not very efficient. You could go on to stack overflow or ask a GPT, what would be an efficient way of joining to a single vector against a column and find and easier way of doing it. Great question. Also, very, very relevant the more that we jump into looking at having very big data sources and things like that. There is a package called Data table, which I'm not very good at it and I don't use it very much, but it is a much faster and more efficient handling of large data under the surface. There is a set of tiverse wrappers now for data table as in a tidyverse package to translate data table functions. We could use data table to do the join and it would be more computationally efficient. Thanks. Really. Cool, in that case, if no one's I'll stop the recording now quickly.