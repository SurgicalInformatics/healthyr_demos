Okay. So there we go. So again, everybody, welcome back to another healthy demo session. I hope everyone is doing well. Today, we're going to be doing a bit of a part two Tidy models demo that we did a few weeks ago this time on to resampling methods within tidy models. So I've got my colleague, Neil here with me, who's currently sharing his screen. So it's the posit cloud space that we're going to be using today, which is a Cloud version of r studio, which you may or may not be familiar with. So Neil, if you would please open the quarto document that called Tidy Models resample. Okay. Yeah, that one there. Perfect. If you close the tab on the left that's posit cloud so that we can just get everything. Okay. What I've got here is the same code and the examples from the Tidy models demo that we had previously, So a bit of it is repetitive, but we're just going to have it so that we can have our recipes and specifications made. Just in case anyone wasn't at that demo or hasn't had time to watch the recording that we've got of that. What I'm going to have you do is the codes already there. But if we can scroll slowly and run the chunks of codes that are going to be coming so that we can just quickly get everybody onto the at the same level for this demo. This first chunk is less about tidy models at all. It was just a bit of data wrangling and preparing the data. So we're going to be looking at outcomes in diabetes is what our dataset was about. So if you can run that chunk  before we proceed, is okay if I switch to sorce view? Oh, of course, yes. That would be a good description about what the environment actually is. Yeah. Okay. Is that okay with everyone? It's just my preferred way of looking at these documents. I'll run this chunk. We're loading tidy, tidy models. We're reading in some data, and we're driving some variables from the data and we're taking a glimpse. There's the glimpse. Here's the type and at look at some of the values that we have. Okay. Yeah. V, some number variables. A lot of that wrangling isn't to be worried about too much because it was basically adding some variables that we then cleaned in our recipe step. That's not super important for this demo, but I just think it's good for the continuation of things to use the same data and things like that. Perfect. Is this is this what we're using to identify individuals? Yeah, that's an ID column there. And then there's some information about each individual. Yeah. And If you run this next chunk, actually, there should be a set seed at the start of that chunk for creating the split for the data. But basically, this chunk here is using some really useful functions in the tiny models where you create a split with initial split and then you can create you can apply the split that you're using. In this case, we're doing a proportion of 0.8, so we're taking 80% versus 20% of our data to split into a training in a test set. What was talking about with setting the seed was because of that element of randomness in that initial split function for creating that split in order to make our analysis reproducible, we should use this function set seed, which makes sure that our The randomness that R is using is always the same when this analysis is run. That was a really poor way of explaining it, I don't know if you have a better way to. Set seed means you have random numbers, but if you set the seed, you can create the same random numbers reproducible random numbers. Should I put that in Sara? Yeah, I think so. Data set demo isn't about that today, but just for general good coding practises, I think it's important to have. Just A number is fine. Any number, I'll just mash the keys. There we go. Perfect. And then I'll run this chunk. Yep. Amazing. As long as in another document, if someone wants to rerun this, if they put in the same number there, you will get the same random numbers and should get the same exact same analysis. Yeah. Otherwise, our training set could or would be different every time, which isn't what you would want if you're publishing your data and you want people to do things to be to copy what you're doing. The next step in tiny models approach is creating a recipe. This is like pre processing our data. It's useful in this tiny models framework because you can create a recipe and apply it to a bunch of different workflows. But here what we're doing is writing our formula that we're going to be working against diabetes binary is our outcome, and we're using our the t is basically against all other variables that we have, and then we're specifying the data set. All of these steps are preprocessing steps, so it's doing something to our data. The step underscored, V is removing the columns that have zero variance, um is creating my variables and update role is basically assigning role to various columns. In this case, we're calling subjects ID. We're giving it any role, it doesn't matter, but basically what we're telling it that it's not variable that we want to adjust for by calling it ID. Basically, when we're using that box variable, that's saying use all our variables because we've called this one an ID one, it knows not to use that in our adjustment set. Perfect, if you run that quickly just to create the recipe. Okay and scroll down. Our specifications, again, it's basically specifying the model that we wanted to use. In our last demo that we set up specifications for both a decision tree and a logistic regression. I'm going through this very fast. We do have another recording on it. So if anyone's feeling overwhelmed, don't worry, you'll be able to watch the other recordings where we go through in a bit more detail. If you run the specifications, No, you can have a look at them if you like. Decision tree. Diabetes, this one. You have a quick look. This is the recipe object. Okay. And then we've created these workflows, which is one of the advantages of the tiny models framework, I think, where you can create a workflow and you specify the recipe that you've created and the model that you've created. But you can really easily switch in different parts so that you can really quickly run multiple analyses all at one. But, if you just run those work flows. And we're almost at where we ended last time with just creating these data metrics and basically pulling out some results from that fi step that we just did in the chunk above that. I kind of skipped over quickly. Augment is basically a function that is useful for predicting whether or not you're Role has, in this instance, diabetes or not, and then you can plot it against the prediction versus against the actual results in free area under the curves and different computer matrices. Just to have a look at the results from your models. Super. An important thing to know when you're doing an analysis. Last time, I think we did talk through this last fit step, which basically grab uses your test so that then you can pull results out from the test set. But you can only use your test set once for kind of for a real analysis. You don't want to be creating a model, training your model, checking your test set, seeing it's not great, going back to your training set, making some changes, and then going back to your test set again. You can't do that. Test set is held out set for that exact purpose. You hold it out till the very end, you train your model, you're happy with it, and then you want to apply on this unseen data to see how your model performs. But if you're only working on one training set, it can be hard to tell how it will work on unseen data. That's where resampling comes into play because it allows us to simulate how our model will perform on data that it's never seen before. To do that, we can do it in various ways. We can use things like boot straps or other methods for creating different sections of your training set. So Neil, That's what I'm going to get you to do is to create some fold first. I've added the link there for our sample tidy models, which should give you a list of different functions for creating all the different folds that are on offer in tidy models. If you could have a look there, and then create an object for the folds for us. Okay. I'm going to look Sorry, I have a little trouble here. Right. So we've got here, resampling methods. Initial split, this is what we've used. So these are splitting methods. Boot straps. Validation. Here is a bunch of the kind of like functions that you might want. I often use boot straps or the kind of fold cross validation. I don't know if you have a preference for what you would I guess, depends on analyses and things, but That's should go for bootstraps. Sounds good. Okay. This is something that we apply to the. We would apply this to the training set the bootstraps. And we've got these automatic values times 25. So it would make by default 25 bootstrap samples, is that correct? And I guess we'll not do stratification. Okay. All right. Okay. So should I create these bootstrap samples? Yeah, that would be perfect. Okay. So yeah, I've got a chunk there where you can create what you want. Okay. So I need to take my training set. I forgot what it was called. Diabetes train. Libet train. Simple as that. There it is. Betes train. And But straps. And should I call this? Is that too long? It does the purpose. It does definitely does what it says in. Perfect. Yes. So run that Yeah. Should I take a quick look at it? Yeah. Okay. So we have 25 rows. So these are 25 bootstrap samples. Here. We have a list object. This is the ID for this, this is a way of labelling the sample. I take a look at this. If there's one thing that tidy models likes, it's a ti within a ti. So it can be quite a lot of lists and things, but. Okay. So access one of these bootstraps. If you highlight or if you just run like Diabetes train bootstraps, I sometimes as not the whole line, but just the object. Yeah, I'll give us a summary which is sometimes a little bit easier to understand. And it's still not shown in the split. What about I just say say the first one. Take a look at it. Oops. Try unnest. I'm not sure. Ops. Okay. Yeah, I'm not quite sure how to do that, sorry. All right. But we've made we've made we've made our bootstrap, nonetheless. We've got 25 splits of our training set. So there's basically what that means is they're going to be chunk of the training set that we use to train on. And then of the same training set, it's be's going to be a validation set when it's in bootstraps, New York. Yes, I think you could call it. Yeah. And then it'll be done 25 times, and then we can basically pull out an average of our metrics so that we can using just our training set, we can calculate how well our model is performing. But now what we have to do is we have to fit these re samples. If you scroll down slightly, I've set up set up the example of how it works with the decision tree. You need to fill in the resamples bit. I was hoping you could fill in that and then also repeat this for the logistic regression workflow that we created earlier. The resamples would be the bootstraps we just created. I can copy that. The resample, we're going to put in the diabits workflow to the decision tree with the resample data control Yeah. That line's a bit confusing. But basically, that line is for saving the predictions. So if you want to be able to create AUC curve from your resamples, you have to add that line in there. In this object that we're creating, it's going to save the predictions as part of that object. So I'll run that. The model has been refit 25 times with these 25 resampling of the data. Yes, you should be able to have a look at that. If you run the diabetes tree or open it, you can have a look at what it's created. That way. Again, a tile within a ti. But that's basically the results for each of the 25 boot straps that we've created. Then this birst bootstrap sample, we've got an accuracy of 0.66, an area in the curve of 0.73 and debris score 0.24. Yeah. That can be done for it. You can cool out those results from each of the boot straps. You can do it the way Neil is doing it here, you can also use the nest function if you're wanting to type it, but I quite enjoy the being able to see it this way. So each one of these is slightly different because the bootstrap samples are slightly different. Exactly. Yeah. Predictions. This is the models, the predicted values for each resample. Yeah, and to have this column saved, that's when you need that pred underscore true variable. But yeah, this allows you to see when the model predicted, yes or no, and what class that is. We've got a 50 50, if it's more than 0.5, it's yes versus no, and then you've got the actual result of the truth of whether or not it was yes or no. And so we've got 1456 rows because that's the size of the data set. Okay. Perfect. And quickly, you'll see that it's given us AUC Brier score and accuracy, those are kind of like the default metrics that it's given us. S you wanted Another metric, for example, sensitivity and specificity. I was thinking it would be a good idea if we showed people how to do that. It's within the Fit resample. It's a argument called metric. I was wondering if you wanted to have Well, you can have a look at the Help tab or whatever is useful, and then or if not, I can help direct you in the direction that you would want. Fit re sample. Should I do this in a new tab or should I just edit this one? You can just edit that one. I didn't do a specific area for it, but yeah. Metrics. Would this be a list? It is a list, but it's not using the combine function. If you go on the Help tab for a fit re samples, I think it's got some good example. You have to use a different function. Called M think it is, but it's worth. Bit resamples? Okay. So metric by default is null. A yardstick metric set. Yeah, that's the one. We want to set it's giving us AUC which I mean, the ones that it's given are quite useful ones to have. But it's just to demonstrate that we could other ones. This is an example of what might look like. Yes, exactly. Metric set. And I can give you an example. For sensitivity and specificity, that's sends and spec, I think are the two then it's like accuracy and then accuracy is just accuracy. But whichever one whatever metrics that you're looking for, you would specify them here in this metric set, then you can either s that as a different object and just say metric equals and then specify your metric set or within the samples, you can type it out here. AUC is actually rock underscore AC. AC. They're all yardstick functions. So its call you see like this. Oh, yeah it does. Lovely. Yes. I then you can run that and you'll be able to see that the output it's the same, but it has those different metrics in it. But it's the familiar output that we just had.pully it don't take too long. There we go. Okay. So I take another look and then look at the metrics. We've got the sensitivity, specificity, accuracy and the area under the rock curve. Great. Yeah. Next thing I want you to do is basically just to show how easy it is to quickly change that to the logistic regression workflow we did. If you just essentially copy and paste, but repistic regression step is what we're on a bit. We had a different work flow up here, forget what it was called. We've worked with this one, the decision tree to do the logistic regression. We just stick in the work flow here. Then I'll change this. Is not tree anymore, but isistic regression. Okay. Okay, so I can take a look at that. We have the same metrics. I could have a quick comparison. We just pick one resampling set. Here we've got an area under the rock curve of 0.73 with the decision tree with the logistic regression is the same. H. Did we definitely change? Let's have a quick check that we changed everything that needs changed? Let progression. Or if you scroll up quickly to see what the work flow is because I might have copied wrong, and that is also a potential. Yes. Was that Lp that? Run it again. That's one important thing to note. When you're changing up, you have to make sure you change it fully. That was a mistake from last time. Super these things again. That one should have changed. Yeah. Sorry. No, sorry. What you were doing was really useful. Actually, just taking a look at some of the resamples and comparing. But there is a handy function within tiny models, which allows us to quickly pull out the averages of the boot straps so that you can also compare them that way. And Is that what we'll do in the next chunk? The next chunk was I set that up for your repeat for logistic regression, but it's down to the evaluate, but I don't it doesn't matter too much. I was just doing that to keep myself make sure I knew what was coming next. So you just skip over that chance. Because we have repeated for logistic regression. We now fit our model on the bootstraps, our logistic regression model. So like N was saying, you might want to have an idea of which model is performed better. That's probably the whole reason why you're running multiple models. And so Tiny models has this really useful function called collect underscore metrics. So if you could run that on both the results from the decision tree and the registic regression, please, N. I'm doing sorry. It doesn't matter too much. There we go and collect. Is this something we're going to want to save for later or we're just examining it now in this chunk? That definitely depends. We don't need to use it in the future chunks in this analysis, but it's definitely something that you might want to save, you could do either. Three metrics. Logistic regression metrics. Super and then give those a run and have a look. Logistic regression metrics. We've got a row for each metric. We've got its mean value, the number of bootstraps that mean was calculated over and the standard error of that mean. In this sense, you can take a look at that. If A C is the metric that you're most interested in, you can see that across those boot straps, the average AC value was 0.82 if you're rounding. That can be a useful metric to have, so you know it performs pretty well. But then we can also look at our tree results and we can have a look at me, same averages, and see which ones best. Uh huh. The tree you see is slightly smaller, ero 0.75 looks like all these metrics are slightly smaller with the tree. That's just a quick handy function for being able to quickly assess the kind of performance. Thing is interesting. I like logistic regression as well. Perfect. Now like our demo last time, you might want to create a curve. You might want to create a visual for your outputs. Instead of using augment to predict to get those predictions together. Instead, we're using this function collect predictions, and then we're piping it into the same old C and auto pop one. I was wondering if you could do that for the tree and for the logistic regression. So we want to do that to resample. Yes, the resamples. So if you're if you're not using your resample results, you don't use collect predictions, you would use Augment. But here, when you're using those resample results, you need to use collect predictions so that it can collect the predictions from all of the boot straps, essentially. We're going to collect predictions together and pass them into the rock. AUC function like this. Yes. We can rock AUC will give us the we can pull out the AUC value, but it's a I've written it wrong here. If we want to plot it, we want to do rock underscore curve. Sorry. Rock curve. Yeah. Then within that, that function, we have to give it the truth, which I can't remember the variable, I think it is called diabetes binary, maybe. Then we also want to give it the column for the predictions. Bites binary? Yes, I think it's that one. Yeah. And then the prediction is p equals? Yes. Or do pred underscore. Yes. It's a capital Y because that's just how we coded our value. So Tru argument is it called prediction? I double check on the function because I usually maybe bad coding practise would just type.pes, but it's worth. So it without explicit. It works without explicitly telling which argument it's called. No columnames. Okay. Just. That's exactly what it's expecting. Si. And then I think you have to type that into the auto plot function. Then that run hopefully. There we go. Great. Then we can repeat that for the logistic ggsion one. Yeah. So it's a little bit larger area, I guess. Yeah. Super. I had I had one more task I should note the kind of functions that we were using for pulling out like the AUC and the confusion matrices and stuff from our last demo or the last tidy models demo work in the same way so that you can or not the confusion matrices, but the ROC Underscore AUC works so we can pull out what the AUC is rather than just pulling out from the collect matrix matrix. But we're running out of time a little bit. But I had one final challenge, but I guess I'll double check if anyone has any questions first, and if they don't, we'll move on to this. But I just don't want anyone to have a clinic question or if you do have a clinic question, put that in the chat. And L at ask are the recordings accessible externally. They are accessible externally. I will I'll post a link to that in just a second. Cool. Okay. If there's no questions coming up, we'll try the last challenge that I had, which was Can we be rock curve with both of those? Yes, to get both of the results in. I've given you and how I would have started that, but I don't know if you've got a different approach to how you would usually do it. I, I don't think I've done it in this way, but one of the things that occurred to me, I ought to take a look at this. What does the rock curve look like? Okay. So one of the ways that I could do this is I could take this table and augment it. I could include a variable that indicates where this rock curve came from, and then I could bind it to another one to the logistic regression one, which I do the same thing on. But then I suppose I would have a table and not a rock curve object to do the auto plot. H This would be one way of doing it. I could say I could say this is the tree rock curve, but I could mutate that to say that the model is equal to tree. Then I would have this is quite clumsy way of doing it. Logistic gresion. This would be. I would have two tiles with the rock curve information. I think you've missed an underscore in the result of the resample, it might Diabetes underscore LR. Oh, yes. So then I can do I could just stick those together. ' and then I can do my own GG put. I still can't remember the names. This will be upside down. Okay. So I've got them both from the same plot, but I didn't want specificity. I weighed one minus specificity. And you just do that in the. Oh, yeah. That's quite a clumsy way of doing it. I'm not using the machinery of tiny bottles there. I've just kind of taken two data frames and mashed them together. There's probably a nicer way of doing that that avoids having to wrangle things. I mean, I think, the method that I had used definitely involved a little bit of wrangling as well. I like this method. If you scroll right to the bottom, I've got an answer of how I did it. It's like literally the very very bottom. I see. Here there, they get them on the same plot. So I was basically just like unnesting the prediction and like you did finding those two rows, creating an extra role called which model it is coping by that role and then just using the rock curve function. But I like your method as well. Both did the job. Yeah, I think a bit goes back a bit further, goes to the predictions and then regenerates the rocker. I see. Yeah. I think that's a general principle is different ways of getting to the same thing with our programme. Yeah, for sure. Lots of different ways to do the same thing. It's all about finding your flow and there's efficiencies here or there, but often it's easy enough to get the same results, which is cool. What I like about your way here is it's just one pipe rather than making two separate objects and then joining them. It's just one process, which is nice. Yeah. Yeah. But yeah. Both do the same job though. I quite like I like GG plot as well because then you'll have the added ease of being able to edit your GG plot in maybe a bit of more familiar way, which the TD models framework is super familiar to everyone. But yeah, brilliant. That's all I had planned for tidy models today. I don't know if anyone has any specific questions on tidy models. I know that was a bit overwhelming, particularly, it's been a few weeks since we did our first Tidy models one and or if people weren't at that first one. But both records this one recording will be available soon. I have realised now I don't think I've actually ticked the B publicly available one to the first Tidy models one. So if you can't see it, kick back this afternoon and it'll be there. And But yes. Perfect. Brilliant. I'm going to start the recording there and